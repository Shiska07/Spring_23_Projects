{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Shiska Raut <br>\n",
    "ID: 1001526329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training/evaluation data\n",
    "\n",
    "**Argument(s):** \n",
    "1) filename: name of a .txt file with each line containing training/evaluation features(x) and label(y) in the following format:\n",
    "((x1, x2, .....xn), y) <br>\n",
    "2) dtype_x : datatype of features <br>\n",
    "3) dtype_y: datatype of label <br>\n",
    "\n",
    "**Return(s):** 'X, Y' where X is a numpy array of feature vectors and Y is the target label vector.\n",
    "Note: Each column in the array(s) epresents a single datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_arrays(filename, dtype_x, dtype_y):\n",
    "    try:\n",
    "        f = open(filename, 'r')\n",
    "    except OSError:\n",
    "        print(f'{filename} could not be opened.\\n')\n",
    "        sys.exit()\n",
    "        \n",
    "    # initialize list to store feature and labels for training data\n",
    "    features = []             \n",
    "    labels = []\n",
    "    \n",
    "    with f:\n",
    "        line = f.readline()\n",
    "        while line != '':\n",
    "            # strip newline and outer parenthesis\n",
    "            line = line.strip('\\n')\n",
    "            line = line.strip('( )')\n",
    "            \n",
    "            # extrace label and append to labels list\n",
    "            single_label = line.split('), ')[-1]\n",
    "            labels.append(single_label)\n",
    "            \n",
    "            # extrace features and append to features list\n",
    "            feat = line.split('), ')[0].split(', ')\n",
    "            features.append(feat)\n",
    "            \n",
    "            # read next line\n",
    "            line = f.readline()\n",
    "        \n",
    "        # create dataframe of features and append labels\n",
    "        X = np.array(features, dtype = dtype_x, ndmin = 2)\n",
    "        \n",
    "        # convert labels list to array\n",
    "        Y = np.array(labels, dtype = dtype_y, ndmin = 2)\n",
    "        \n",
    "        return X.transpose(), Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read test data\n",
    "\n",
    "**Argument(s):** \n",
    "1) name of a .txt file with each line containing test features(x) in the following format:\n",
    "(x1, x2, .....xn) <br>\n",
    "2) dtype_x: datatype of features\n",
    "\n",
    "**Return(s):** 'X' where X is a numpy array of feature vectors.\n",
    "Note: Each column in the array(s) epresents a single datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_array(filename, dtype_x):\n",
    "    try:\n",
    "        f = open(filename, 'r')\n",
    "    except OSError:\n",
    "        print(f'{filename} could not be opened.\\n')\n",
    "        sys.exit()\n",
    "        \n",
    "    # initialize list to store feature and labels for training data\n",
    "    features = []             \n",
    "    \n",
    "    with f:\n",
    "        line = f.readline()\n",
    "        while line != '':\n",
    "            \n",
    "            # get feature values\n",
    "            line = line.strip('\\n')\n",
    "            line = line.strip('( )')\n",
    "            feat = line.split(', ')\n",
    "            features.append(feat)\n",
    "            \n",
    "            # read next line\n",
    "            line = f.readline()\n",
    "        \n",
    "        # create dataframe of features and append labels\n",
    "        X = np.array(features, dtype = dtype_x, ndmin = 2)\n",
    "        \n",
    "        return X.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one_hot_encoder(arr) : return encoded_arr, label_idx_dict\n",
    "**arr:** <br>\n",
    "[['Ceramic' 'Metal' 'Metal' 'Metal' 'Ceramic' 'Plastic' 'Plastic'\n",
    "  'Plastic' 'Plastic' 'Plastic' 'Plastic' 'Ceramic']]<br>  \n",
    "**encoded_arr:** <br>\n",
    "[[0 1 1 1 0 0 0 0 0 0 0 0]<br>\n",
    " [0 0 0 0 0 1 1 1 1 1 1 0]<br>\n",
    " [1 0 0 0 1 0 0 0 0 0 0 1]] <br>\n",
    "**label_idx_dict:** <br>\n",
    "{'Metal': 0, 'Plastic': 1, 'Ceramic': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an array of attribute values for a categocial attribute,\n",
    "# preforms one-hot-encoding and returns resulting binary array\n",
    "def one_hot_encoder(arr):\n",
    "    \n",
    "    __, n_samples = arr.shape\n",
    "    \n",
    "    # get unique labels\n",
    "    uniq_labels = set(arr[0,:].tolist())\n",
    "    \n",
    "    # get number of total attribute values\n",
    "    n_labels = len(uniq_labels)\n",
    "    \n",
    "    # create an array of size n_labels*n_samples to store encoded values\n",
    "    encoded_arr = np.zeros((n_labels, n_samples), dtype = int)\n",
    "    \n",
    "    # create dictionary to store row indev of each attribute value\n",
    "    label_idx_dict = {}\n",
    "    for i, v in enumerate(uniq_labels):\n",
    "        label_idx_dict[v] = i\n",
    "        \n",
    "    # fill encoded_arr using attribute index dictionary and input arr\n",
    "    for i in range(n_samples):\n",
    "        # get index to encode as 1\n",
    "        idx = label_idx_dict[arr[0,i]]\n",
    "        encoded_arr[idx, i] = 1\n",
    "        \n",
    "    return encoded_arr, label_idx_dict\n",
    "\n",
    "# adds bias as the first row to a dataset\n",
    "def add_bias(X):\n",
    "    \n",
    "    n_feat, n_samples = X.shape\n",
    "    X_b = np.ones((n_feat+1, n_samples), dtype = float)\n",
    "    X_b[1::,:] = X\n",
    "    \n",
    "    return X_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 a) Functions for Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compares two arrays and returns accuracy\n",
    "def get_acc(Y_pred, Y):\n",
    "    n_samples = Y_pred.shape[1]\n",
    "    return (np.sum(Y_eval == Y_pred))/n_samples\n",
    "\n",
    "# given a vector of parobaility values, returns label with max probability for a single sample\n",
    "def get_sample_prediction_label(sfmax_net, label_idx_dict):\n",
    "    \n",
    "    # get inverse of the dictionary\n",
    "    inv_label_idx_dict = {v: k for k, v in label_idx_dict.items()}\n",
    "    \n",
    "    # return label with max probability value\n",
    "    return inv_label_idx_dict[int(np.argmax(sfmax_net, axis = 0))]\n",
    "\n",
    "\n",
    "# uses softmax function and parameter matrix to get probability values\n",
    "# for multiclass classification\n",
    "def get_sample_prediction_values(x, model_params):\n",
    "    \n",
    "    # initialize column vector of ones to add bias\n",
    "    x_sample = np.ones((x.shape[0]+1, 1), dtype = float)\n",
    "    x_sample[1::] = x.reshape(x.shape[0], 1)\n",
    "    \n",
    "    # calculate linear net value\n",
    "    net = np.dot(model_params, x_sample)\n",
    "    \n",
    "    # calculate exponential value for rach class\n",
    "    exp_net = np.exp(net)\n",
    "    \n",
    "    # calculate softmax value for each class\n",
    "    sfmax_net = exp_net/np.sum(exp_net, axis = 0)\n",
    "    \n",
    "    return sfmax_net\n",
    "\n",
    "# gets predictions for an entire test dataset\n",
    "def get_predictions(X_test, model_params, label_idx_dict):\n",
    "\n",
    "    # initialize list to store predictions\n",
    "    Y_pred = []\n",
    "\n",
    "    # get number of test samples\n",
    "    __, n_samples = X_test.shape\n",
    "\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        y_pred_values = get_sample_prediction_values(X_test[:,i], model_params)\n",
    "        y_pred_label = get_sample_prediction_label(y_pred_values, label_idx_dict)\n",
    "        Y_pred.append(y_pred_label)\n",
    "\n",
    "    # convert labels list to numpy array\n",
    "    Y_pred = np.array(Y_pred, dtype = str, ndmin = 2)\n",
    "\n",
    "    return Y_pred  \n",
    "\n",
    "\n",
    "# trains a softmax regression model given training data, alpha and number of epochs\n",
    "# with batch gradient descent\n",
    "def train_softmax_regressor_batch(X_train, Y_train, alpha, epochs):\n",
    "    \n",
    "    # get number of features and samples\n",
    "    n_feat, n_samples = X_train.shape\n",
    "    \n",
    "    # get no of classes/labels\n",
    "    n_class, __ = Y_train.shape\n",
    "\n",
    "    # get paramater matrix\n",
    "    model_params = np.random.uniform(-0.01, 0.01, size = (n_class, n_feat))\n",
    "\n",
    "    # initialize list to store net change in parameter values\n",
    "    epoch_change_model_params = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # initialize gradient vector for each epoch\n",
    "        gradient_mtx = np.zeros((n_class, n_feat), dtype = float)\n",
    "\n",
    "        for j in range(n_samples):\n",
    "            \n",
    "            # pick a sample \n",
    "            x_sample = X_train[:,j].reshape(n_feat,1)\n",
    "            y_sample = Y_train[:,j].reshape(n_class, 1)\n",
    "            \n",
    "            # get prediction value\n",
    "            y_pred = get_sample_prediction_values(x_sample, model_params)\n",
    "\n",
    "            # calculate gradient matrix\n",
    "            sample_gradient = np.dot((y_sample - y_pred), x_sample.transpose())\n",
    "            gradient_mtx = gradient_mtx + sample_gradient\n",
    "            \n",
    "        # adjust parameter values using batch gradient descent \n",
    "        updated_params = model_params + (alpha*gradient_mtx)\n",
    "        \n",
    "        # get the net change in parameters\n",
    "        net_change = np.sum(np.abs(model_params - updated_params))\n",
    "        epoch_change_model_params.append(net_change)\n",
    "            \n",
    "        # set updated parameters as new parameters  \n",
    "        model_params = updated_params.copy()\n",
    "        \n",
    "    # return final parameter matrix\n",
    "    return model_params, epoch_change_model_params\n",
    "\n",
    "# trains a softmax regression model given training data, alpha and number of epochs\n",
    "# with stochastic gradient descent\n",
    "def train_softmax_regressor_stochastic(X_train, Y_train, alpha, epochs):\n",
    "    \n",
    "    # get number of features and samples\n",
    "    n_feat, n_samples = X_train.shape\n",
    "    \n",
    "    # get no of classes/labels\n",
    "    n_class, __ = Y_train.shape\n",
    "\n",
    "    # get paramater matrix\n",
    "    model_params = np.random.uniform(0, 0.01, size = (n_class, n_feat))\n",
    "    model_params_init = model_params.copy()\n",
    "\n",
    "    # initialize list to store net change in parameter values\n",
    "    epoch_change_model_params = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        for j in range(n_samples):\n",
    "            \n",
    "            # pick a sample randomly\n",
    "            idx = np.random.randint(0, n_samples)\n",
    "            x_sample = X_train[:,idx].reshape(n_feat,1)\n",
    "            y_sample = Y_train[:,idx].reshape(n_class, 1)\n",
    "            \n",
    "            # get prediction value\n",
    "            y_pred = get_sample_prediction_values(x_sample, model_params)\n",
    "\n",
    "            # calculate gradient matrix\n",
    "            sample_gradient = np.dot((y_sample - y_pred), x_sample.transpose())\n",
    "            \n",
    "            # adjust parameter values using batch gradient descent \n",
    "            model_params = model_params + (alpha*sample_gradient)\n",
    "\n",
    "        # get the net change in parameters\n",
    "        net_change = np.sum(np.abs(model_params_init - model_params))\n",
    "        change_model_params.append(net_change)\n",
    "\n",
    "        # set updated parameters as new parameters  \n",
    "        model_params_init = model_params.copy()\n",
    "        \n",
    "    # return final parameter matrix\n",
    "    return model_params, epoch_change_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Filenames:\n",
    "1) Training/evaluation file: name of a .txt file with each line containing training/evaluation features(x) and label(y) in the following format:\n",
    "((x1, x2, .....xn), y)\n",
    "\n",
    "2) Test file: name of a .txt file with each line containing test features(x) in the following format:\n",
    "(x1, x2, .....xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fname_train = str(input('Enter file containing training data: '))\n",
    "fname_test = str(input('Enter file containing test data: '))\n",
    "'''\n",
    "fname_train = 'Data/3_train.txt'\n",
    "fname_test = 'Data/3_test.txt'\n",
    "epochs = 1000\n",
    "alpha = 0.001\n",
    "\n",
    "X_train, Y_train = get_X_Y_arrays(fname_train, float, str)\n",
    "X_test = get_X_array(fname_test, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias to X fro training and test data\n",
    "X_train_b = add_bias(X_train)\n",
    "X_test_b = add_bias(X_test)\n",
    "\n",
    "# get encoded array for y\n",
    "Y_train_encoded, label_idx_dict = one_hot_encoder(Y_train)\n",
    "\n",
    "# train model and get predictions\n",
    "model_params = train_softmax_regressor(X_train, Y_train, alpha, epochs)\n",
    "Y_pred = get_predictions(X_test, model_params, label_idx_dict)\n",
    "acc = get_acc(Y_pred, Y_test)\n",
    "print(f'Test accuracy is {acc:.3f}.\\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 b) Function for leave-one-out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_evaluation(X_eval, Y_eval, alpha, epochs):\n",
    "    \n",
    "    # get number of features and samples\n",
    "    n_feat, n_samples = X_eval.shape\n",
    "    \n",
    "    # prediction labels generated by 'predict_class_with_knn' will be stored in this list\n",
    "    Y_pred = []\n",
    "\n",
    "    # add bias to X\n",
    "    X_train_b = np.ones((n_feat+1, n_samples), dtype = float)\n",
    "    X_train_b[1::,:] = X_eval\n",
    "\n",
    "    # get encoded array for y\n",
    "    Y_train_encoded, label_idx_dict = one_hot_encoder(Y_eval)\n",
    "    \n",
    "    # trains a new model to predict each sample\n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        # pick test datapoint\n",
    "        x_test = X_train_b[:,i].reshape(n_feat+1, 1)\n",
    "        y_test = Y_train_encoded[:,i].reshape(Y_train_encoded.shape[0], 1)\n",
    "        \n",
    "        # create traiing set by deleting test datapoint\n",
    "        X_train = np.delete(X_train_b, i, axis = 1)\n",
    "        Y_train = np.delete(Y_train_encoded, i, axis = 1)\n",
    "        \n",
    "        # train model\n",
    "        model_params = train_softmax_regressor(X_train, Y_train, alpha, epochs)\n",
    "        \n",
    "        # get test data prediction\n",
    "        y_pred_values = get_sample_prediction_values(x_test, model_params)\n",
    "        y_pred_label = get_sample_prediction_label(y_pred_values, label_idx_dict)\n",
    "        Y_pred.append(y_pred_label)\n",
    "    \n",
    "    # convert prediction list to numpy array\n",
    "    Y_pred = np.array(Y_pred, dtype = str, ndmin = 2)\n",
    "    acc = (np.sum(Y_eval == Y_pred))/n_samples\n",
    "    \n",
    "    # return predictions and accuracy\n",
    "    return Y_pred, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.hist(Y_eval.squeeze())\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Evaluation after removing 4th attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0021400603194769944\n",
      "-0.0021400603194769944\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "i = 5\n",
    "x_sample = 2.5\n",
    "print(((np.cos(i*k*x_sample))**(i*k))*np.cos(x_sample))\n",
    "print((np.cos(i*k*x_sample)**(i*k))*np.cos(x_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
